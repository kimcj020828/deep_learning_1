'''
교차 엔트로피 오차를 구하는 방법은 다음과 같다
E = -np.sum((t*np.log(y)))
=> 정답에 해당하는 출력이 커질수록 y축은 0에 가까워 진다.
 

데이터 분석 시 로그를 사용하는 이유?
데이터 분석을 하기 위해 log를 취하는 이유는 한마디로 정규성을 높이고 분석(회귀분석 등)에서 정확한 값을 얻기 위함이다.

데이터 간 편차를 줄여 왜도(skewness-데이터가 한쪽으로 치우친 정도)와 첨도2(Kurtosis-분포가 얼마나 뾰족한지를 나타내는 정도)를 줄일 수 있기 때문에 정규성이 높아진다.

예를 들어, 연령 같은 경우에는 숫자의 범위가 약 0세~120세 이하 이겠지만, 재산 보유액 같은 경우에는 0원에서 몇 조단위까지 올라갈 수 있다. 즉, 데이터 간 단위가 달라지면 결과값이 이상해 질 수 있다.

log의 역할은 큰 수를 같은 비율의 작은 수로 바꿔 주는 것이다.

log는 큰 수를 작게 만들고 복잡한 계산을 간편하게 하기위해 사용한다. 로그를 취하는 순간 그 수는 지수가 되어버리니, 값이 작아 진다.
'''
import numpy as np
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
# delta를 더하는 이유는 log함수에 0이 들어가게 되면 무한대의 값이 출력되기 때문에 0이 되는 것을 방지하기 위함이다.

t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
print(cross_entropy_error(y,t))# 0.510825457099338

y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])
print(cross_entropy_error(y,t)) #2.302584092994546

'''
앞의 오차제곱합의 결과와 같은 판단을 함(계산의 결과가 클수록 정답과 거리가 멀다.)
'''